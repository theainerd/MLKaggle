{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](https://i.pinimg.com/originals/40/b1/3d/40b13d00e57b21a195217db15e03403e.png)\n",
    "\n",
    "This kernel will introduce you to the basics of text processing with spaCy. If you are a beginner like me then for sure you will find this guide helpful. \n",
    "\n",
    "The idea will be to learn basic preprocessing using spaCy which is an Industrial-Strength Natural Language Processing library and then level up based on the problems we are trying to solve. As this is my first kernel I hope I don't mess up a lot.\n",
    "\n",
    "You can also take this course in detail by spaCy : [Advanced NLP with spaCy](https://course.spacy.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['news-headlines-dataset-for-sarcasm-detection', 'spacyen-vectors-web-lg']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use News Headlines dataset for Sarcasm Detection Dataset.\n",
    "You can find the Dataset here : [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (26709, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "train = train.drop(['article_link'], axis=1)\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can apply machine learning algorithms we have to preprocess our text data. Here's how to clean your text data.\n",
    "\n",
    "- Remove all irrelevant characters such as any non alphanumeric characters\n",
    "- Tokenize your text by separating it into individual words\n",
    "- Remove words that are not relevant, such as “@” twitter mentions or urls(if any)\n",
    "- Convert all characters to lowercase(**Case folding**), in order to treat words such as “hello”, “Hello”, and “HELLO” the same. ******\n",
    "- Consider combining misspelled or alternately spelled words to a single representation (e.g. “cool”/”kewl”/”cooool”)\n",
    "- Consider lemmatization (reduce words such as “am”, “are”, and “is” to a common form such as “be”)\n",
    "- Consider removing stopwords (such as a, an, the, be)etc.\n",
    "\n",
    "**Note** : For tasks like speech recognition and information retrieval, everything is mapped to lower case. For sentiment analysis and other text classification tasks, information extraction, and machine translation, by contrast, case is quite helpful and case folding is generally not done (losing the difference, for example, between **US the country and us the pronoun** can outweigh the advantage in generality that case folding provides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review is:\n",
      "\n",
      " former versace store clerk sues over secret 'black code' for minority shoppers\n"
     ]
    }
   ],
   "source": [
    "# Check the first review\n",
    "\n",
    "print('The first review is:\\n\\n',train[\"headline\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create cleanData function. \n",
    "This function will remove stopwords, punctuations, convert text to lowercase and preform lemmatization.\n",
    "\n",
    "**Stopwords**: Stop words are words that are particularly common in a text corpus and thus considered as rather un-informative.\n",
    "\n",
    "**Lemmatization**: Lemmatization refers to normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean data\n",
    "\n",
    "def cleanData(doc,stemming = False):\n",
    "    doc = doc.lower()\n",
    "    doc = nlp(doc)\n",
    "    tokens = [tokens.lower_ for tokens in doc]\n",
    "    tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n",
    "    tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n",
    "    final_token = [token.lemma_ for token in tokens]\n",
    "    \n",
    "    return \" \".join(final_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versace store clerk sue secret black code minority shopper'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_review = cleanData(train['headline'][0])\n",
    "clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning train data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean description\n",
    "print(\"Cleaning train data...\\n\")\n",
    "train[\"headline\"] = train[\"headline\"].map(lambda x: cleanData(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of spaCy\n",
    "\n",
    "**spaCy** is a free, open-source library for advanced Natural Language Processing (NLP) in Python. \n",
    "\n",
    "spaCy is designed specifically for **production use** and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to **pre-process text for deep learning**.\n",
    "\n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. “I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, now the co-founder and CEO of online higher education startup Udacity, in an interview with Recode earlier this week.\n",
    "\n",
    "A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.\"\"\"\n",
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is really happening here is that  we pass a string of text to the nlp object, and receive a Doc object.\n",
    "\n",
    "![](https://course.spacy.io/pipeline.png)\n",
    "\n",
    "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. After tokenization, spaCy can parse and tag a given Doc.\n",
    "There are several preprocessing tasks which we will go through one by one.\n",
    "The best thing about spaCy pipeline is that you can always **add custom functions** in this pipeline depending on your problem.\n",
    "\n",
    "Some of the basic functions are:\n",
    "\n",
    "* Tokenization\n",
    "* Part-of-speech (POS) Tagging\n",
    "* Named Entity Recognition (NER) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "> Tokenization is the process of converting a sequence of characters into a sequence of tokens.\n",
    "\n",
    "![](https://raw.githubusercontent.com/theainerd/MLInterview/master/images/Screenshot%20from%202018-10-04%2014-14-08.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\n",
      "Sebastian\n",
      "Thrun\n",
      "started\n",
      "working\n",
      "on\n",
      "self\n",
      "-\n",
      "driving\n",
      "cars\n",
      "at\n",
      "Google\n",
      "in\n",
      "2007\n",
      ",\n",
      "few\n",
      "people\n",
      "outside\n",
      "of\n",
      "the\n",
      "company\n",
      "took\n",
      "him\n",
      "seriously\n",
      ".\n",
      "“\n",
      "I\n",
      "can\n",
      "tell\n",
      "you\n",
      "very\n",
      "senior\n",
      "CEOs\n",
      "of\n",
      "major\n",
      "American\n",
      "car\n",
      "companies\n",
      "would\n",
      "shake\n",
      "my\n",
      "hand\n",
      "and\n",
      "turn\n",
      "away\n",
      "because\n",
      "I\n",
      "was\n",
      "n’t\n",
      "worth\n",
      "talking\n",
      "to\n",
      ",\n",
      "”\n",
      "said\n",
      "Thrun\n",
      ",\n",
      "now\n",
      "the\n",
      "co\n",
      "-\n",
      "founder\n",
      "and\n",
      "CEO\n",
      "of\n",
      "online\n",
      "higher\n",
      "education\n",
      "startup\n",
      "Udacity\n",
      ",\n",
      "in\n",
      "an\n",
      "interview\n",
      "with\n",
      "Recode\n",
      "earlier\n",
      "this\n",
      "week\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "little\n",
      "less\n",
      "than\n",
      "a\n",
      "decade\n",
      "later\n",
      ",\n",
      "dozens\n",
      "of\n",
      "self\n",
      "-\n",
      "driving\n",
      "startups\n",
      "have\n",
      "cropped\n",
      "up\n",
      "while\n",
      "automakers\n",
      "around\n",
      "the\n",
      "world\n",
      "clamor\n",
      ",\n",
      "wallet\n",
      "in\n",
      "hand\n",
      ",\n",
      "to\n",
      "secure\n",
      "their\n",
      "place\n",
      "in\n",
      "the\n",
      "fast\n",
      "-\n",
      "moving\n",
      "world\n",
      "of\n",
      "fully\n",
      "automated\n",
      "transportation\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# print each token\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts-of-Speech (POS) tagging\n",
    "\n",
    "> *Part-of-speech tagging (POS tagging)* is the task of tagging a word in a text with its part of speech. A part of speech is a category of words with similar grammatical properties. Common English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc.\n",
    "\n",
    "the *\"en_core_web_sm\"* package is a small English model that supports all core capabilities and is trained on web text. The package provides the **binary weights** that enable spaCy to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When ADV\n",
      "Sebastian PROPN\n",
      "Thrun PROPN\n",
      "started VERB\n",
      "working VERB\n",
      "on ADP\n",
      "self NOUN\n",
      "- PUNCT\n",
      "driving VERB\n",
      "cars NOUN\n",
      "at ADP\n",
      "Google PROPN\n",
      "in ADP\n",
      "2007 NUM\n",
      ", PUNCT\n",
      "few ADJ\n",
      "people NOUN\n",
      "outside ADV\n",
      "of ADP\n",
      "the DET\n",
      "company NOUN\n",
      "took VERB\n",
      "him PRON\n",
      "seriously ADV\n",
      ". PUNCT\n",
      "“ PUNCT\n",
      "I PRON\n",
      "can VERB\n",
      "tell VERB\n",
      "you PRON\n",
      "very ADV\n",
      "senior ADJ\n",
      "CEOs NOUN\n",
      "of ADP\n",
      "major ADJ\n",
      "American ADJ\n",
      "car NOUN\n",
      "companies NOUN\n",
      "would VERB\n",
      "shake VERB\n",
      "my DET\n",
      "hand NOUN\n",
      "and CCONJ\n",
      "turn VERB\n",
      "away ADV\n",
      "because ADP\n",
      "I PRON\n",
      "was VERB\n",
      "n’t ADV\n",
      "worth ADJ\n",
      "talking VERB\n",
      "to ADP\n",
      ", PUNCT\n",
      "” PUNCT\n",
      "said VERB\n",
      "Thrun PROPN\n",
      ", PUNCT\n",
      "now ADV\n",
      "the DET\n",
      "co NOUN\n",
      "- NOUN\n",
      "founder NOUN\n",
      "and CCONJ\n",
      "CEO NOUN\n",
      "of ADP\n",
      "online ADJ\n",
      "higher ADJ\n",
      "education NOUN\n",
      "startup NOUN\n",
      "Udacity PROPN\n",
      ", PUNCT\n",
      "in ADP\n",
      "an DET\n",
      "interview NOUN\n",
      "with ADP\n",
      "Recode PROPN\n",
      "earlier ADV\n",
      "this DET\n",
      "week NOUN\n",
      ". PUNCT\n",
      "\n",
      "\n",
      " SPACE\n",
      "A DET\n",
      "little ADJ\n",
      "less ADJ\n",
      "than ADP\n",
      "a DET\n",
      "decade NOUN\n",
      "later ADV\n",
      ", PUNCT\n",
      "dozens NOUN\n",
      "of ADP\n",
      "self NOUN\n",
      "- PUNCT\n",
      "driving VERB\n",
      "startups NOUN\n",
      "have VERB\n",
      "cropped VERB\n",
      "up PART\n",
      "while ADP\n",
      "automakers NOUN\n",
      "around ADP\n",
      "the DET\n",
      "world NOUN\n",
      "clamor NOUN\n",
      ", PUNCT\n",
      "wallet NOUN\n",
      "in ADP\n",
      "hand NOUN\n",
      ", PUNCT\n",
      "to PART\n",
      "secure VERB\n",
      "their DET\n",
      "place NOUN\n",
      "in ADP\n",
      "the DET\n",
      "fast ADV\n",
      "- PUNCT\n",
      "moving VERB\n",
      "world NOUN\n",
      "of ADP\n",
      "fully ADV\n",
      "automated VERB\n",
      "transportation NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "> In the Named Entity Recognition (NER) task, systems are required to recognize the Named Entities occurring in the text. More specifically, the task is to find Person (PER), Organization (ORG), Location (LOC) and Geo-Political Entities (GPE). For instance, in the statement ”Shyam lives in India”, NER system extracts Shyam which refers to name of the person and India which refers to name of the country.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Udacity PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n",
      "A little less than a decade later TIME\n",
      "dozens CARDINAL\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "# Get quick definitions of the most common tags and labels\n",
    "\n",
    "print(spacy.explain('GPE'))\n",
    "print(spacy.explain('ORG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working on self-driving cars at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously. “I can tell you very senior CEOs of major \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " car companies would shake my hand and turn away because I wasn’t worth talking to,” said \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", now the co-founder and CEO of online higher education startup \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Udacity\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", in an interview with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Recode\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    earlier this week\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</br></br>\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    A little less than a decade later\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    dozens\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding custom functions to pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be added using the `nlp.add_pipe` method. You can also mention where you want to add the component using argumenets.\n",
    "\n",
    "Let's add a component of word counts to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print('Doc length:', len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print('Pipeline:', nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everytime you create a new nlp object it will always print the document length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 124\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document similarity\n",
    "\n",
    "spaCy can compare two objects and predict similarity.  In order to use similarity, you need a larger spaCy model that has word vectors included.\n",
    "\n",
    "For example, the medium or large English model – but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"My name is shyam\")\n",
    "doc2 = nlp(\"My name is Ram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents similarity is: 0.8383951915960476\n"
     ]
    }
   ],
   "source": [
    "print(\"The documents similarity is:\" ,doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a very basic guide to get started on NLP for beginners, we saw data cleaning for text data and how to use spaCy for different Natural Language Processing Tasks.\n",
    "\n",
    "References:\n",
    "\n",
    "[Advanced NLP with spaCy](https://course.spacy.io/)\n",
    "\n",
    "Kernels you can explore:\n",
    "\n",
    "[Hitchhiker's Guide to NLP in spaCy](https://www.kaggle.com/nirant/hitchhiker-s-guide-to-nlp-in-spacy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
